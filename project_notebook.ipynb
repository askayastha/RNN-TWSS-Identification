{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cps680_project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ph7XO0oZ1EfL",
        "CdslUdfndFHo",
        "42_oqfwQWOVz"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjjSTBFiWe5N"
      },
      "source": [
        "# Get TWSS data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nluCdePu3pA4"
      },
      "source": [
        "!mkdir -p /opt/data/\n",
        "!wget -q https://github.com/tansaku/twss/archive/refs/heads/master.zip -O /opt/data/twss.zip\n",
        "!unzip -q /opt/data/twss.zip -d /opt/data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcJD19hO39RZ",
        "outputId": "7c766000-d768-43ea-8c20-850dc0e655f1"
      },
      "source": [
        "!ls /opt/data/twss-master/data/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fml.txt\t\t     svm_model.pk\t  twssSample1.txt     usaquotes.txt.pk\n",
            "fml.txt.pk\t     test.pk\t\t  twssSample2.txt     vocab.pk\n",
            "README.txt\t     tfln.onesent.txt\t  twssstories.txt     vocab.txt\n",
            "sentenceSample1.txt  tfln.onesent.txt.pk  twssstories.txt.pk\n",
            "sentenceSample2.txt  train.pk\t\t  usaquotes.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScACNKKvfYGu"
      },
      "source": [
        "# Organize the files in 'pos' and 'neg' directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGkQqzs94NoI",
        "outputId": "df7c1141-fd2e-4798-d81a-7274ba5d457f"
      },
      "source": [
        "!mkdir -p /opt/data/pos/\n",
        "!mkdir -p /opt/data/neg/\n",
        "!cp /opt/data/twss-master/data/twssstories.txt /opt/data/pos/\n",
        "!cp /opt/data/twss-master/data/fml.txt /opt/data/neg/\n",
        "!cp /opt/data/twss-master/data/tfln.onesent.txt /opt/data/neg/\n",
        "!cp /opt/data/twss-master/data/usaquotes.txt /opt/data/neg/\n",
        "!ls /opt/data/pos/\n",
        "!ls /opt/data/neg/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "twssstories.txt\n",
            "fml.txt  tfln.onesent.txt  usaquotes.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4UBvutiWoBo"
      },
      "source": [
        "# Read data as samples and targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmgrGV2Dsjjn"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "TWSS_DIR = '/opt/data/'\n",
        "\n",
        "labels = []\n",
        "texts = []\n",
        "\n",
        "# Change is_balanced to True for all preprocessing except imbalanced training data\n",
        "is_balanced = True\n",
        "\n",
        "for label_type in ['neg', 'pos']:\n",
        "  dir_name = os.path.join(TWSS_DIR, label_type)\n",
        "  for fname in os.listdir(dir_name):\n",
        "    if fname[-4:] == '.txt':\n",
        "      f = open(os.path.join(dir_name, fname), encoding='latin1')\n",
        "      for i, line in enumerate(f.readlines()):\n",
        "        # Uncomment for balanced data\n",
        "        if i >= 669 and label_type == 'neg' and is_balanced:\n",
        "          texts.append(line.rstrip())\n",
        "          labels.append(2)\n",
        "          continue\n",
        "        texts.append(line.rstrip())\n",
        "        if label_type == 'neg':\n",
        "          labels.append(0)\n",
        "        else:\n",
        "          labels.append(1)\n",
        "      f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYru03M9AvJ_",
        "outputId": "603878d9-ec4d-4c23-d335-b185449e12b2"
      },
      "source": [
        "print(\"Total Samples:\")\n",
        "print(\"Balanced:\", is_balanced)\n",
        "print(len(texts))\n",
        "print(len(labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Samples:\n",
            "Balanced: True\n",
            "19863\n",
            "19863\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KhjvBhbO0TV"
      },
      "source": [
        "# Pick one of the class balance:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVtl2HkpWU_I"
      },
      "source": [
        "# Preprocessing with balanced training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsA6ELCX5fll",
        "outputId": "5078e9ae-125e-4d8d-d05a-e5b6884e3990"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "MAX_LEN = 100\n",
        "MAX_WORDS = 15000\n",
        "CLASS_BALANCE = 'Balanced'\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# print(sequences[:5])\n",
        "# for s in sequences[:5]:\n",
        "#   print(len(s))\n",
        "\n",
        "# print([*word_index][:5])\n",
        "\n",
        "all_data = pad_sequences(sequences, maxlen=MAX_LEN, padding='post')\n",
        "all_labels = np.asarray(labels)\n",
        "\n",
        "data_indices = [index for index, element in enumerate(all_labels) if element == 0 or element == 1]\n",
        "\n",
        "data = all_data[np.asarray(data_indices)]\n",
        "labels = all_labels[np.asarray(data_indices)]\n",
        "\n",
        "# Shuffle data\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "# Train-test split\n",
        "TEST_SPLIT = 0.8\n",
        "N = round(len(data) * TEST_SPLIT)\n",
        "\n",
        "train_X = data[:N]\n",
        "train_Y = labels[:N]\n",
        "test_X = data[N:]\n",
        "test_Y = labels[N:]\n",
        "\n",
        "# Verify the shape of the data\n",
        "print(train_X.shape)\n",
        "print(train_Y.shape)\n",
        "print(test_X.shape)\n",
        "print(test_Y.shape)\n",
        "\n",
        "# print(data[:5])\n",
        "# for s in data[:5]:\n",
        "#   print(len(s))\n",
        "\n",
        "# print(labels[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3227, 100)\n",
            "(3227,)\n",
            "(807, 100)\n",
            "(807,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPnOxdu4xgND",
        "outputId": "1ce01265-6ae7-4605-8cec-88810a556832"
      },
      "source": [
        "print(\"Neg training samples count:\", np.count_nonzero(train_Y==0))\n",
        "print(\"Pos training samples count:\", np.count_nonzero(train_Y==1))\n",
        "\n",
        "print(\"Neg test samples count:\", np.count_nonzero(test_Y==0))\n",
        "print(\"Pos test samples count:\", np.count_nonzero(test_Y==1))\n",
        "\n",
        "print(\"Number of tokens: \", len(word_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Neg training samples count: 1617\n",
            "Pos training samples count: 1610\n",
            "Neg test samples count: 390\n",
            "Pos test samples count: 417\n",
            "Number of tokens:  20758\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph7XO0oZ1EfL"
      },
      "source": [
        "# Preprocessing with imbalanced training data but balanced test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Y7HaBIX1CYQ",
        "outputId": "469091ff-3eb7-423c-82a0-e71b5bd388c4"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "MAX_LEN = 100\n",
        "MAX_WORDS = 15000\n",
        "CLASS_BALANCE = 'Only Test Balanced'\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# print(sequences[:5])\n",
        "# for s in sequences[:5]:\n",
        "#   print(len(s))\n",
        "\n",
        "# print([*word_index][:5])\n",
        "\n",
        "all_data = pad_sequences(sequences, maxlen=MAX_LEN, padding='post')\n",
        "all_labels = np.asarray(labels)\n",
        "\n",
        "data_indices = [index for index, element in enumerate(all_labels) if element == 0 or element == 1]\n",
        "extra_data_indices = [index for index, element in enumerate(all_labels) if element == 2]\n",
        "\n",
        "data = all_data[np.asarray(data_indices)]\n",
        "extra_data = all_data[np.asarray(extra_data_indices)]\n",
        "print(\"Extra Data Length:\", extra_data.shape)\n",
        "\n",
        "labels = all_labels[np.asarray(data_indices)]\n",
        "extra_labels = all_labels[np.asarray(extra_data_indices)]\n",
        "print(\"Extra Data Labels Length:\", extra_labels.shape)\n",
        "\n",
        "# Change temp label '2' back to '0'\n",
        "extra_labels = np.where(extra_labels == 2, 0, extra_labels)\n",
        "\n",
        "# Shuffle data\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "# Train-test split\n",
        "TRAIN_SPLIT = 0.9\n",
        "N = round(TRAIN_SPLIT * data.shape[0])\n",
        "\n",
        "train_X = data[:N, :]\n",
        "train_Y = labels[:N]\n",
        "test_X = data[N:, :]\n",
        "test_Y = labels[N:]\n",
        "\n",
        "# Pull the last 10% as a validation set\n",
        "# N = round(TRAIN_SPLIT * train_X.shape[0])\n",
        "# val_X = train_X[N:, :]\n",
        "# val_Y = train_Y[N:]\n",
        "# train_X = train_X[:N, :]\n",
        "# train_Y = train_Y[:N]\n",
        "\n",
        "# Verify the shape of the data\n",
        "print(train_X.shape)\n",
        "print(train_Y.shape)\n",
        "print(test_X.shape)\n",
        "print(test_Y.shape)\n",
        "# print(val_X.shape)\n",
        "# print(val_Y.shape)\n",
        "\n",
        "train_X = np.concatenate((train_X, extra_data))\n",
        "train_Y = np.concatenate((train_Y, extra_labels))\n",
        "\n",
        "print(train_X.shape)\n",
        "print(train_Y.shape)\n",
        "\n",
        "# print(data[:5])\n",
        "# for s in data[:5]:\n",
        "#   print(len(s))\n",
        "\n",
        "# print(labels[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extra Data Length: (15829, 100)\n",
            "Extra Data Labels Length: (15829,)\n",
            "(3631, 100)\n",
            "(3631,)\n",
            "(403, 100)\n",
            "(403,)\n",
            "(19460, 100)\n",
            "(19460,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kj9MeMWbFHEZ",
        "outputId": "cab449c2-1532-49b3-9732-b1cf329c0500"
      },
      "source": [
        "print(\"Neg training samples count:\", np.count_nonzero(train_Y==0))\n",
        "print(\"Pos training samples count:\", np.count_nonzero(train_Y==1))\n",
        "\n",
        "print(\"Neg test samples count:\", np.count_nonzero(test_Y==0))\n",
        "print(\"Pos test samples count:\", np.count_nonzero(test_Y==1))\n",
        "\n",
        "# print(\"Neg val samples count:\", np.count_nonzero(val_Y ==0))\n",
        "# print(\"Pos val samples count:\", np.count_nonzero(val_Y==1))\n",
        "\n",
        "print(\"Number of tokens: \", len(word_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Neg training samples count: 17639\n",
            "Pos training samples count: 1821\n",
            "Neg test samples count: 197\n",
            "Pos test samples count: 206\n",
            "Number of tokens:  20758\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdslUdfndFHo"
      },
      "source": [
        "# Preprocessing with imbalanced training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdjLm9DhdB-g",
        "outputId": "aaae4b04-4dcc-4e24-9a1a-be5a6b66cb8f"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "MAX_LEN = 100\n",
        "MAX_WORDS = 15000\n",
        "CLASS_BALANCE = 'Imbalanced'\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# print(sequences[:5])\n",
        "# for s in sequences[:5]:\n",
        "#   print(len(s))\n",
        "\n",
        "# print([*word_index][:5])\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=MAX_LEN, padding='post')\n",
        "labels = np.asarray(labels)\n",
        "\n",
        "# Shuffle data\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "# Train-test split\n",
        "TEST_SPLIT = 0.8\n",
        "N = round(len(data) * TEST_SPLIT)\n",
        "\n",
        "train_X = data[:N]\n",
        "train_Y = labels[:N]\n",
        "test_X = data[N:]\n",
        "test_Y = labels[N:]\n",
        "\n",
        "# Verify the shape of the data\n",
        "print(train_X.shape)\n",
        "print(train_Y.shape)\n",
        "print(test_X.shape)\n",
        "print(test_Y.shape)\n",
        "\n",
        "# print(data[:5])\n",
        "# for s in data[:5]:\n",
        "#   print(len(s))\n",
        "\n",
        "# print(labels[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(15890, 100)\n",
            "(15890,)\n",
            "(3973, 100)\n",
            "(3973,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DW_L-GwnFufz",
        "outputId": "3b1e3232-e243-4dc6-d237-098d324c562d"
      },
      "source": [
        "print(\"Neg training samples count:\", np.count_nonzero(train_Y==0))\n",
        "print(\"Pos training samples count:\", np.count_nonzero(train_Y==1))\n",
        "\n",
        "print(\"Neg test samples count:\", np.count_nonzero(test_Y==0))\n",
        "print(\"Pos test samples count:\", np.count_nonzero(test_Y==1))\n",
        "\n",
        "print(\"Number of tokens: \", len(word_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Neg training samples count: 14272\n",
            "Pos training samples count: 1618\n",
            "Neg test samples count: 3564\n",
            "Pos test samples count: 409\n",
            "Number of tokens:  20758\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQH-tqN_VlrF"
      },
      "source": [
        "# Setup GloVe Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZ-BCkfzEBSP",
        "outputId": "3b4fed12-b4a3-41b4-e766-19e29cf71580"
      },
      "source": [
        "# Get the GloVe embeddings\n",
        "!mkdir -p /opt/data/\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip -O /opt/data/glove.6B.zip\n",
        "!unzip /opt/data/glove.6B.zip -d /opt/data/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-12 17:58:11--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-05-12 17:58:11--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-05-12 17:58:12--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘/opt/data/glove.6B.zip’\n",
            "\n",
            "/opt/data/glove.6B. 100%[===================>] 822.24M  5.26MB/s    in 2m 41s  \n",
            "\n",
            "2021-05-12 18:00:52 (5.12 MB/s) - ‘/opt/data/glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  /opt/data/glove.6B.zip\n",
            "  inflating: /opt/data/glove.6B.50d.txt  \n",
            "  inflating: /opt/data/glove.6B.100d.txt  \n",
            "  inflating: /opt/data/glove.6B.200d.txt  \n",
            "  inflating: /opt/data/glove.6B.300d.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKG8OvGJEEas",
        "outputId": "9670453e-c209-4140-c13d-049ddcaa803d"
      },
      "source": [
        "!ls /opt/data/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "glove.6B.100d.txt  glove.6B.300d.txt  glove.6B.zip  pos\t\t twss.zip\n",
            "glove.6B.200d.txt  glove.6B.50d.txt   neg\t    twss-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQ8mB79fEHPF",
        "outputId": "705bf048-992d-426d-88c0-40c96ce569ba"
      },
      "source": [
        "import os\n",
        "\n",
        "BASE_DIR = '/opt/data/'\n",
        "EMBEDDING_DIM = 300\n",
        "GLOVE_DIR = os.path.join(BASE_DIR, f'glove.6B.{EMBEDDING_DIM}d.txt')\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(GLOVE_DIR) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xsaDQZbLpYs"
      },
      "source": [
        "# embeddings_index['apple']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcfaYNWLVtpm"
      },
      "source": [
        "# Encode TWSS words into embedding vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hLTzYMcgicZ"
      },
      "source": [
        "# Use all the all the words in the training data (Optional)\n",
        "MAX_WORDS = len(word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a10KX2crMBJS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98771155-9466-43c1-c466-53e91177fb57"
      },
      "source": [
        "embedding_matrix = np.zeros((MAX_WORDS, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "  if i < MAX_WORDS:\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      embedding_matrix[i] = embedding_vector\n",
        "\n",
        "embedding_matrix.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15000, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waT4zNYWqoeA"
      },
      "source": [
        "# Define helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODfU8VMeqRC4"
      },
      "source": [
        "def print_results(results, name):\n",
        "  print(f\"\\n{name} Results\")\n",
        "  print('-' * (len(name) + 1 + len(\"Results\")))\n",
        "  print(f\"Loss: {results[0]:.2%}\")\n",
        "  print(f\"Precision: {results[1]:.2%}\")\n",
        "  print(f\"Recall: {results[2]:.2%}\")\n",
        "  print(f\"AUC: {results[3]:.2%}\")\n",
        "  print(f\"Accuracy: {results[4]:.2%}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTPu5UJIVzzB"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dczFqCYdEhsO"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "PRETRAINED_GLOVE = True\n",
        "MASKING = True\n",
        "RNN_TYPE = 'GRU'\n",
        "NEURAL_UNITS = 128\n",
        "DROPOUT = 0.2\n",
        "OPTIMIZER = 'Adam'\n",
        "LEARNING_RATE = 0.001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4X4aPF73MGBp",
        "outputId": "9dff289e-d24d-42b2-e433-6df0404ac85f"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense, Embedding, Dropout\n",
        "from keras.metrics import Precision, Recall, BinaryAccuracy, AUC\n",
        "from keras.layers import LSTM, GRU, Bidirectional, Masking\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "if RNN_TYPE == 'None':\n",
        "  model.add(Embedding(MAX_WORDS, 50, mask_zero=MASKING, input_length=MAX_LEN))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(64, activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "else:\n",
        "  model.add(Embedding(MAX_WORDS, EMBEDDING_DIM, mask_zero=MASKING))\n",
        "  model.add(GRU(NEURAL_UNITS, dropout=DROPOUT))\n",
        "  # model.add(Bidirectional(GRU(NEURAL_UNITS, dropout=DROPOUT)))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "if PRETRAINED_GLOVE:\n",
        "  model.layers[0].set_weights([embedding_matrix])\n",
        "  model.layers[0].trainable = False\n",
        "\n",
        "model.summary()\n",
        "model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss='binary_crossentropy', metrics=[Precision(), Recall(), AUC(), BinaryAccuracy()])\n",
        "es = EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=3)\n",
        "\n",
        "# For imbalanced training data but balanced test and validation data\n",
        "# history = model.fit(train_X, train_Y, epochs=10, batch_size=32, validation_data=(val_X, val_Y))\n",
        "\n",
        "# Validation split\n",
        "# history = model.fit(train_X, train_Y, epochs=100, batch_size=32,  callbacks=[es], validation_split=0.1)\n",
        "# history = model.fit(train_X, train_Y, epochs=10, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# No validation split\n",
        "# history = model.fit(train_X, train_Y, epochs=100, batch_size=32,  callbacks=[es])\n",
        "history = model.fit(train_X, train_Y, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
        "\n",
        "# print(model.predict(data[:10,:]))\n",
        "# print(labels[:10])\n",
        "results = model.evaluate(test_X, test_Y)\n",
        "print_results(results, \"Test\")\n",
        "\n",
        "PRECISION = round(results[1]*100, 2)\n",
        "RECALL = round(results[2]*100, 2)\n",
        "AUC = round(results[3]*100, 2)\n",
        "ACCURACY = round(results[4]*100, 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_10 (Embedding)     (None, 100, 50)           750000    \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 5000)              0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 64)                320064    \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 1,070,129\n",
            "Trainable params: 1,070,129\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "609/609 [==============================] - 7s 10ms/step - loss: 0.2246 - precision_10: 0.6348 - recall_10: 0.2118 - auc_10: 0.8382 - binary_accuracy: 0.9233\n",
            "Epoch 2/10\n",
            "609/609 [==============================] - 6s 10ms/step - loss: 0.0564 - precision_10: 0.8943 - recall_10: 0.8735 - auc_10: 0.9938 - binary_accuracy: 0.9788\n",
            "Epoch 3/10\n",
            "609/609 [==============================] - 6s 10ms/step - loss: 0.0259 - precision_10: 0.9542 - recall_10: 0.9580 - auc_10: 0.9984 - binary_accuracy: 0.9915\n",
            "Epoch 4/10\n",
            "609/609 [==============================] - 6s 10ms/step - loss: 0.0090 - precision_10: 0.9911 - recall_10: 0.9899 - auc_10: 0.9998 - binary_accuracy: 0.9982\n",
            "Epoch 5/10\n",
            "609/609 [==============================] - 6s 10ms/step - loss: 0.0044 - precision_10: 0.9962 - recall_10: 0.9984 - auc_10: 0.9998 - binary_accuracy: 0.9995\n",
            "Epoch 6/10\n",
            "609/609 [==============================] - 6s 10ms/step - loss: 0.0032 - precision_10: 0.9958 - recall_10: 0.9985 - auc_10: 0.9999 - binary_accuracy: 0.9995\n",
            "Epoch 7/10\n",
            "609/609 [==============================] - 6s 10ms/step - loss: 9.7851e-04 - precision_10: 0.9994 - recall_10: 0.9995 - auc_10: 1.0000 - binary_accuracy: 0.9999\n",
            "Epoch 8/10\n",
            "609/609 [==============================] - 6s 10ms/step - loss: 0.0016 - precision_10: 0.9975 - recall_10: 0.9978 - auc_10: 1.0000 - binary_accuracy: 0.9996\n",
            "Epoch 9/10\n",
            "609/609 [==============================] - 6s 10ms/step - loss: 8.8418e-04 - precision_10: 0.9989 - recall_10: 0.9990 - auc_10: 1.0000 - binary_accuracy: 0.9998\n",
            "Epoch 10/10\n",
            "609/609 [==============================] - 6s 10ms/step - loss: 0.0011 - precision_10: 0.9985 - recall_10: 0.9984 - auc_10: 1.0000 - binary_accuracy: 0.9997\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.7846 - precision_10: 0.9653 - recall_10: 0.8107 - auc_10: 0.9473 - binary_accuracy: 0.8883\n",
            "\n",
            "Test Results\n",
            "------------\n",
            "Loss: 78.46%\n",
            "Precision: 96.53%\n",
            "Recall: 81.07%\n",
            "AUC: 94.73%\n",
            "Accuracy: 88.83%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqxEq-AuZ612"
      },
      "source": [
        "print(history.history.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44ddWgs8DAqi"
      },
      "source": [
        "# Save the results in Google Sheets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RE87Zu-GC_lC"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMMlcICUDf7L"
      },
      "source": [
        "sh = gc.create('TWSS Results')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-hxN6rPDWY5"
      },
      "source": [
        "worksheet = gc.open('TWSS Results').sheet1\n",
        "row_list = [MAX_WORDS, EMBEDDING_DIM, PRETRAINED_GLOVE, MASKING, RNN_TYPE, NEURAL_UNITS, DROPOUT, CLASS_BALANCE, OPTIMIZER, LEARNING_RATE, BATCH_SIZE, PRECISION, RECALL, AUC, ACCURACY]\n",
        "\n",
        "model_entry = worksheet.insert_row(row_list, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42_oqfwQWOVz"
      },
      "source": [
        "# Visualize test evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDceycKsrgXI"
      },
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "preds_Y = model.predict(test_X)\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(test_Y, preds_Y)\n",
        "\n",
        "plt.plot(recall, precision, marker='.', label='RNN Word Embedding')\n",
        "plt.title(f'Precision-Recall Curve')\n",
        "# axis labels\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "# show the legend\n",
        "plt.legend()\n",
        "plt.savefig(\"best_model_pr-rc.png\", dpi=256, format='png')\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUcqOvW4Z2Hl"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "hist_keys = list(history.history.keys())\n",
        "\n",
        "def plot_acc_loss(history):\n",
        "  history_dict = history.history\n",
        "\n",
        "  acc = history_dict[hist_keys[1]]\n",
        "  val_acc = history_dict[hist_keys[6]]\n",
        "  loss = history_dict['loss']\n",
        "  val_loss = history_dict['val_loss']\n",
        "\n",
        "  epochs = range(1, len(acc) + 1)\n",
        "\n",
        "  plt.plot(epochs, acc, 'bo', label=f'Accuracy')\n",
        "  plt.plot(epochs, val_acc, 'r', label=f'Validation Accuracy')\n",
        "  plt.title(f'Training and validation accuracy')\n",
        "  plt.legend()\n",
        "\n",
        "  # plt.savefig(\"best_model_acc.png\", dpi=144, format='png')\n",
        "  plt.figure()\n",
        "\n",
        "  plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "  plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "  plt.title('Training and validation loss')\n",
        "  plt.legend()\n",
        "\n",
        "  # plt.savefig(\"best_model_loss.png\", dpi=144, format='png')\n",
        "  plt.show()\n",
        "\n",
        "plot_acc_loss(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usRkwW7hh9jM"
      },
      "source": [
        "test_X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyeJt_6BbCPd"
      },
      "source": [
        "# Mount Google Drive and setup paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPk7MUz1M2oW"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giyVKZmokY50"
      },
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "TWSS_MODELS_DIR = os.path.join('/content/drive', 'MyDrive', 'TWSS', 'models')\n",
        "\n",
        "TWSS_TEST_DATA_DIR = os.path.join('/content/drive', 'MyDrive', 'TWSS', 'test_data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "np6HEiR_kiXf"
      },
      "source": [
        "# Save the model and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EY5rx5oRbHKf"
      },
      "source": [
        "TWSS_MODEL_FILENAME = model_entry['updatedRange'].replace('!', '-') + '.h5'\n",
        "TWSS_MODEL_PATH = os.path.join(TWSS_MODELS_DIR, TWSS_MODEL_FILENAME)\n",
        "TWSS_TEST_DATA_FILENAME = model_entry['updatedRange'].replace('!', '-') + '.pkl'\n",
        "TWSS_TEST_DATA_PATH = os.path.join(TWSS_TEST_DATA_DIR, TWSS_TEST_DATA_FILENAME)\n",
        "\n",
        "model.save(TWSS_MODEL_PATH)\n",
        "\n",
        "test_data = {\n",
        "    'test_X': test_X,\n",
        "    'test_Y': test_Y\n",
        "}\n",
        "\n",
        "with open(TWSS_TEST_DATA_PATH, 'wb') as f:\n",
        "  pickle.dump(test_data, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UL68NUFbImI"
      },
      "source": [
        "# Load the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e60qss2bMqe"
      },
      "source": [
        "from keras import models\n",
        "\n",
        "best_model_names = {\n",
        "    'Sheet1-A3:O3': 'GloVe w/ GRU (Train/Test Balanced)',\n",
        "    'Sheet1-A4:O4': 'GloVe w/ GRU (Only Test Balanced)',\n",
        "    'Sheet1-A2:O2': 'GloVe w/ GRU (Train/Test Imbalanced)',\n",
        "    'Sheet1-A6:O6': 'No GloVe w/ GRU (Train/Test Imbalanced)',\n",
        "    'Sheet1-A5:O5': 'No GloVe w/o GRU (Train/Test Imbalanced)'\n",
        "    # 'Sheet1-A7:O7': 'Balanced No Glove RNN',\n",
        "    # 'Sheet1-A8:O8': 'Balanced No Glove No RNN'\n",
        "    # 'Sheet1-A9:O9': 'Val/Test No Glove',\n",
        "    # 'Sheet1-A10:O10': 'Val/Test No Glove No RNN'\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8cPxsLSuhfL"
      },
      "source": [
        "!mv times-new-roman.ttf /usr/share/fonts/truetype/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYjY5yxN_tO5"
      },
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "path = '/usr/share/fonts/truetype/times-new-roman.ttf'\n",
        "fontprop1 = fm.FontProperties(fname=path, size=14)\n",
        "fontprop2 = fm.FontProperties(fname=path, size=13)\n",
        "\n",
        "for model_name in best_model_names.keys():\n",
        "  twss_model_path = os.path.join(TWSS_MODELS_DIR, f'{model_name}.h5')\n",
        "  twss_model = models.load_model(twss_model_path)\n",
        "  twss_test_data_path = os.path.join(TWSS_TEST_DATA_DIR, f'{model_name}.pkl')\n",
        "\n",
        "  with open(twss_test_data_path, 'rb') as f:\n",
        "    test_data = pickle.load(f)\n",
        "\n",
        "  preds_Y = twss_model.predict(test_data['test_X'])\n",
        "  precision, recall, thresholds = precision_recall_curve(test_data['test_Y'], preds_Y)\n",
        "  plt.plot(recall, precision, linestyle='solid', label=best_model_names[model_name])\n",
        "\n",
        "# plt.title(f'Precision-Recall Curve')\n",
        "# axis labels\n",
        "\n",
        "plt.grid(linestyle='--', color='lightgray')\n",
        "plt.xticks(np.arange(0, 1, 0.1), fontproperties=fontprop1)\n",
        "plt.yticks(np.arange(0, 1, 0.1), fontproperties=fontprop1)\n",
        "plt.xlabel('Recall', fontproperties=fontprop1)\n",
        "plt.ylabel('Precision', fontproperties=fontprop1)\n",
        "# show the legend\n",
        "plt.legend(prop=fontprop2)\n",
        "plt.savefig(\"precision_recall.png\", dpi=256, format='png')\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmmIG_T2V6DZ"
      },
      "source": [
        "# Test the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gklzKVRH3ZSx"
      },
      "source": [
        "twss_model_path = os.path.join(TWSS_MODELS_DIR, 'Sheet1-A3:O3.h5')\n",
        "model = models.load_model(twss_model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHxlgwnOeP9L",
        "outputId": "506ca644-9be4-47b9-8500-4ecc46e27885"
      },
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "sample_twss = OrderedDict({\n",
        "    \"Wow! I cannot believe it. This is much bigger than I thought it would be.\": 1,\n",
        "    \"He’s under a lot of pressure, which builds up until he’s ready to explode. It’s my job to release that pressure.\": 1,\n",
        "    \"Please don't make it harder than it has to be.\": 1,\n",
        "    \"I was trying all night yesterday, but I couldn't get it in.\": 1,\n",
        "    \"I fear the people will not quietly submit to those restraints which are necessary for the peace and security of the community.\": 0,\n",
        "    \"I am going to go home tonight and find a place to put this.\": 1,\n",
        "    \"I will be completely honest with you, I wanted this so bad I could taste it.\": 1,\n",
        "    \"Thank You! This is a true honour and I'm glad that I came.\": 1,\n",
        "    \"I love eating apples, bananas and mango.\": 0,\n",
        "    \"I love eating bananas.\": 1,\n",
        "    \"I like banana trees.\": 0,\n",
        "    \"I love reading books.\": 0,\n",
        "    \"I spilled water all over the floor.\": 0,\n",
        "    \"You really think you can go all day long.\": 1,\n",
        "    \"Well, you always left me satisfied and smiling.\": 1,\n",
        "    \"Well, you always left me angry.\": 0,\n",
        "    \"Why did you get it so big?\": 1,\n",
        "    \"Why did you get it so late?\": 0,\n",
        "    \"Does the skin look red and swollen?\": 1,\n",
        "    \"That thing looks red and swollen\": 1,\n",
        "    \"That thing looks long, red and swollen.\": 1,\n",
        "    \"You already did me!\": 1,\n",
        "    \"I can't stay on top of you 24/7.\": 1,\n",
        "    \"You don't need to be banging that hard.\": 1,\n",
        "    \"You don't need to bang that hard.\": 1,\n",
        "    \"Don't bang on the door that hard.\": 0,\n",
        "    \"Long, hard and fast.\": 1,\n",
        "    \"Can you grab my banana for a while?\": 1,\n",
        "    \"Don’t you think these buns are a little too big for this meat?\": 1,\n",
        "    \"No matter what you have heard, size matters.\": 1,\n",
        "    \"No matter what you have heard, grade matters.\": 0,\n",
        "    \"Do you want to play with my joystick?\": 1,\n",
        "    \"Do you want to grab lunch?\": 0,\n",
        "})\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(list(sample_twss.keys()))\n",
        "twss_data = pad_sequences(sequences, maxlen=MAX_LEN)\n",
        "\n",
        "sample_results = model.evaluate(twss_data, np.array(list(sample_twss.values())))\n",
        "print_results(sample_results, \"Sample Test\")\n",
        "\n",
        "predictions = model.predict(twss_data)\n",
        "\n",
        "for i, sample in enumerate(sample_twss):\n",
        "  print(f'\\n{sample} => {predictions[i][0]*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 8ms/step - loss: 2.8200 - precision_10: 0.4444 - recall_10: 0.1739 - auc_10: 0.2935 - binary_accuracy: 0.2727\n",
            "\n",
            "Sample Test Results\n",
            "-------------------\n",
            "Loss: 282.00%\n",
            "Precision: 44.44%\n",
            "Recall: 17.39%\n",
            "AUC: 29.35%\n",
            "Accuracy: 27.27%\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f9d93e26e60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "\n",
            "Wow! I cannot believe it. This is much bigger than I thought it would be. => 0.48%\n",
            "\n",
            "He’s under a lot of pressure, which builds up until he’s ready to explode. It’s my job to release that pressure. => 98.04%\n",
            "\n",
            "Please don't make it harder than it has to be. => 0.13%\n",
            "\n",
            "I was trying all night yesterday, but I couldn't get it in. => 1.09%\n",
            "\n",
            "I fear the people will not quietly submit to those restraints which are necessary for the peace and security of the community. => 99.83%\n",
            "\n",
            "I am going to go home tonight and find a place to put this. => 92.56%\n",
            "\n",
            "I will be completely honest with you, I wanted this so bad I could taste it. => 9.88%\n",
            "\n",
            "Thank You! This is a true honour and I'm glad that I came. => 71.79%\n",
            "\n",
            "I love eating apples, bananas and mango. => 57.28%\n",
            "\n",
            "I love eating bananas. => 44.63%\n",
            "\n",
            "I like banana trees. => 35.74%\n",
            "\n",
            "I love reading books. => 77.15%\n",
            "\n",
            "I spilled water all over the floor. => 55.64%\n",
            "\n",
            "You really think you can go all day long. => 1.61%\n",
            "\n",
            "Well, you always left me satisfied and smiling. => 10.56%\n",
            "\n",
            "Well, you always left me angry. => 34.16%\n",
            "\n",
            "Why did you get it so big? => 0.20%\n",
            "\n",
            "Why did you get it so late? => 2.28%\n",
            "\n",
            "Does the skin look red and swollen? => 77.35%\n",
            "\n",
            "That thing looks red and swollen => 5.56%\n",
            "\n",
            "That thing looks long, red and swollen. => 1.75%\n",
            "\n",
            "You already did me! => 4.76%\n",
            "\n",
            "I can't stay on top of you 24/7. => 15.93%\n",
            "\n",
            "You don't need to be banging that hard. => 0.12%\n",
            "\n",
            "You don't need to bang that hard. => 0.23%\n",
            "\n",
            "Don't bang on the door that hard. => 0.50%\n",
            "\n",
            "Long, hard and fast. => 0.02%\n",
            "\n",
            "Can you grab my banana for a while? => 48.85%\n",
            "\n",
            "Don’t you think these buns are a little too big for this meat? => 0.06%\n",
            "\n",
            "No matter what you have heard, size matters. => 22.44%\n",
            "\n",
            "No matter what you have heard, grade matters. => 90.67%\n",
            "\n",
            "Do you want to play with my joystick? => 1.32%\n",
            "\n",
            "Do you want to grab lunch? => 4.78%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avIy6H6gWJIH"
      },
      "source": [
        "# Evaluate extra data (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms0Mqyrn9DyM"
      },
      "source": [
        "print(f\"Extra Data Length: {len(extra_data)}\", end='\\n\\n')\n",
        "print(model.evaluate(extra_data, extra_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZWEBEqXU4z8"
      },
      "source": [
        "model.save('test_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}